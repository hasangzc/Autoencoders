{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03770398",
   "metadata": {},
   "source": [
    "* In the last few years, deep learning based generative models have gained more and more interest due to (and implying) some amazing improvements in the field. Relying on huge amount of data, well-designed networks architectures and smart training techniques, deep generative models have shown an incredible ability to produce highly realistic pieces of content of various kind, such as images, texts and sounds. Among these deep generative models, two major families stand out and deserve a special attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca9d4a",
   "metadata": {},
   "source": [
    "* In a nutshell, a VAE is an autoencoder whose encodings distribution is regularised during the training in order to ensure that its latent space has good properties allowing us to generate some new data. Moreover, the term “variational” comes from the close relation there is between the regularisation and the variational inference method in statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fe5c5",
   "metadata": {},
   "source": [
    "## Dimensionality reduction, PCA and autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754627e5",
   "metadata": {},
   "source": [
    "### What is dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd644908",
   "metadata": {},
   "source": [
    "* In machine learning, dimensionality reduction is the process of reducing the number of features that describe some data. This reduction is done either by selection (only some existing features are conserved) or by extraction (a reduced number of new features are created based on the old features) and can be useful in many situations that require low dimensional data (data visualisation, data storage, heavy computation…). Although there exists many different methods of dimensionality reduction, we can set a global framework that is matched by most (if not any!) of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7645d",
   "metadata": {},
   "source": [
    "* First, let’s call encoder the process that produce the “new features” representation from the “old features” representation (by selection or by extraction) and decoder the reverse process. Dimensionality reduction can then be interpreted as data compression where the encoder compress the data (from the initial space to the encoded space, also called latent space) whereas the decoder decompress them. Of course, depending on the initial data distribution, the latent space dimension and the encoder definition, this compression can be lossy, meaning that a part of the information is lost during the encoding process and cannot be recovered when decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082ba46",
   "metadata": {},
   "source": [
    "<img src=\"./img/vae/vae1.png\" height=\"700\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42fc6d",
   "metadata": {},
   "source": [
    "* The main purpose of a dimensionality reduction method is to find the best encoder/decoder pair among a given family. In other words, for a given set of possible encoders and decoders, we are looking for the pair that keeps the maximum of information when encoding and, so, has the minimum of reconstruction error when decoding. If we denote respectively E and D the families of encoders and decoders we are considering, then the dimensionality reduction problem can be written"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc1fd0",
   "metadata": {},
   "source": [
    "<img src=\"./img/vae/vae2.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "where\n",
    "\n",
    "<img src=\"./img/vae/vae3.png\" height=\"400\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee2a9b",
   "metadata": {},
   "source": [
    "## Principal components analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7848e",
   "metadata": {},
   "source": [
    "* One of the first methods that come in mind when speaking about dimensionality reduction is principal component analysis (PCA).\n",
    "* The idea of PCA is to build n_e new independent features that are linear combinations of the n_d old features and so that the projections of the data on the subspace defined by these new features are as close as possible to the initial data (in term of euclidean distance). In other words, PCA is looking for the best linear subspace of the initial space (described by an orthogonal basis of new features) such that the error of approximating the data by their projections on this subspace is as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f11c7",
   "metadata": {},
   "source": [
    "<img src=\"./img/vae/vae4.png\" height=\"700\" width=\"700\">\n",
    "\n",
    "* Translated in our global framework, we are looking for an encoder in the family E of the n_e by n_d matrices (linear transformation) whose rows are orthonormal (features independence) and for the associated decoder among the family D of n_d by n_e matrices. It can be shown that the unitary eigenvectors corresponding to the n_e greatest eigenvalues (in norm) of the covariance features matrix are orthogonal (or can be chosen to be so) and define the best subspace of dimension n_e to project data on with minimal error of approximation. Thus, these n_e eigenvectors can be chosen as our new features and, so, the problem of dimension reduction can then be expressed as an eigenvalue/eigenvector problem. Moreover, it can also be shown that, in such case, the decoder matrix is the transposed of the encoder matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70305315",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503cd9e",
   "metadata": {},
   "source": [
    "* Let’s now discuss autoencoders and see how we can use neural networks for dimensionality reduction. The general idea of autoencoders is pretty simple and consists in setting an encoder and a decoder as neural networks and to learn the best encoding-decoding scheme using an iterative optimisation process. So, at each iteration we feed the autoencoder architecture (the encoder followed by the decoder) with some data, we compare the encoded-decoded output with the initial data and backpropagate the error through the architecture to update the weights of the networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420c9a1",
   "metadata": {},
   "source": [
    "* Thus, intuitively, the overall autoencoder architecture (encoder+decoder) creates a bottleneck for data that ensures only the main structured part of the information can go through and be reconstructed. Looking at our general framework, the family E of considered encoders is defined by the encoder network architecture, the family D of considered decoders is defined by the decoder network architecture and the search of encoder and decoder that minimise the reconstruction error is done by gradient descent over the parameters of these networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf79e36",
   "metadata": {},
   "source": [
    "<img src=\"./img/vae/vae5.png\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f356e",
   "metadata": {},
   "source": [
    "* Let’s first suppose that both our encoder and decoder architectures have only one layer without non-linearity (linear autoencoder). Such encoder and decoder are then simple linear transformations that can be expressed as matrices. In such situation, we can see a clear link with PCA in the sense that, just like PCA does, we are looking for the best linear subspace to project data on with as few information loss as possible when doing so. Encoding and decoding matrices obtained with PCA define naturally one of the solutions we would be satisfied to reach by gradient descent, but we should outline that this is not the only one. Indeed, several basis can be chosen to describe the same optimal subspace and, so, several encoder/decoder pairs can give the optimal reconstruction error. Moreover, for linear autoencoders and contrarily to PCA, the new features we end up do not have to be independent (no orthogonality constraints in the neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d988752",
   "metadata": {},
   "source": [
    "* Now, let’s assume that both the encoder and the decoder are deep and non-linear. In such case, the more complex the architecture is, the more the autoencoder can proceed to a high dimensionality reduction while keeping reconstruction loss low. Intuitively, if our encoder and our decoder have enough degrees of freedom, we can reduce any initial dimensionality to 1. Indeed, an encoder with “infinite power” could theoretically takes our N initial data points and encodes them as 1, 2, 3, … up to N (or more generally, as N integer on the real axis) and the associated decoder could make the reverse transformation, with no loss during the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd6491",
   "metadata": {},
   "source": [
    "* Here, we should however keep two things in mind. First, an important dimensionality reduction with no reconstruction loss often comes with a price: the lack of interpretable and exploitable structures in the latent space (lack of regularity). Second, most of the time the final purpose of dimensionality reduction is not to only reduce the number of dimensions of the data but to reduce this number of dimensions while keeping the major part of the data structure information in the reduced representations. For these two reasons, the dimension of the latent space and the “depth” of autoencoders (that define degree and quality of compression) have to be carefully controlled and adjusted depending on the final purpose of the dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3a099",
   "metadata": {},
   "source": [
    "<img src=\"./img/vae/vae6.png\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03600b9c",
   "metadata": {},
   "source": [
    "# Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96e4816",
   "metadata": {},
   "source": [
    "###  Limitations of autoencoders for content generation\n",
    "* At this point, a natural question that comes in mind is “what is the link between autoencoders and content generation?”. Indeed, once the autoencoder has been trained, we have both an encoder and a decoder but still no real way to produce any new content. At first sight, we could be tempted to think that, if the latent space is regular enough (well “organized” by the encoder during the training process), we could take a point randomly from that latent space and decode it to get a new content. The decoder would then act more or less like the generator of a Generative Adversarial Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6b935",
   "metadata": {},
   "source": [
    "<img src=\"./img/vae/vae7.png\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac305b",
   "metadata": {},
   "source": [
    "* However, as we discussed in the previous section, the regularity of the latent space for autoencoders is a difficult point that depends on the distribution of the data in the initial space, the dimension of the latent space and the architecture of the encoder. So, it is pretty difficult (if not impossible) to ensure, a priori, that the encoder will organize the latent space in a smart way compatible with the generative process we just described."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e293059",
   "metadata": {},
   "source": [
    "* When thinking about it for a minute, this lack of structure among the encoded data into the latent space is pretty normal. Indeed, nothing in the task the autoencoder is trained for enforce to get such organisation: the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised. Thus, if we are not careful about the definition of the architecture, it is natural that, during the training, the network takes advantage of any overfitting possibilities to achieve its task as well as it can… unless we explicitly regularise it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95bbf6",
   "metadata": {},
   "source": [
    "## Definition of variational autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a6002",
   "metadata": {},
   "source": [
    "* So, in order to be able to use the decoder of our autoencoder for generative purpose, we have to be sure that the latent space is regular enough. One possible solution to obtain such regularity is to introduce explicit regularisation during the training process. Thus, as we briefly mentioned in the introduction of this post, a variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae871749",
   "metadata": {},
   "source": [
    "* Just as a standard autoencoder, a variational autoencoder is an architecture composed of both an encoder and a decoder and that is trained to minimise the reconstruction error between the encoded-decoded data and the initial data. However, in order to introduce some regularisation of the latent space, we proceed to a slight modification of the encoding-decoding process: instead of encoding an input as a single point, we encode it as a distribution over the latent space. The model is then trained as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77924695",
   "metadata": {},
   "source": [
    "** first, the input is encoded as distribution over the latent space\n",
    "\n",
    "\n",
    "** second, a point from the latent space is sampled from that distribution\n",
    "\n",
    "\n",
    "** third, the sampled point is decoded and the reconstruction error can be computed\n",
    "\n",
    "\n",
    "** finally, the reconstruction error is backpropagated through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600495fd",
   "metadata": {},
   "source": [
    "<img src=\"./img/vae/vae8.png\" height=\"800\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac94c15",
   "metadata": {},
   "source": [
    "* In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. The reason why an input is encoded as a distribution with some variance instead of a single point is that it makes possible to express very naturally the latent space regularisation: the distributions returned by the encoder are enforced to be close to a standard normal distribution. We will see in the next subsection that we ensure this way both a local and global regularisation of the latent space (local because of the variance control and global because of the mean control)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f36498",
   "metadata": {},
   "source": [
    "* Thus, the loss function that is minimised when training a VAE is composed of a “reconstruction term” (on the final layer), that tends to make the encoding-decoding scheme as performant as possible, and a “regularisation term” (on the latent layer), that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution. That regularisation term is expressed as the Kulback-Leibler divergence between the returned distribution and a standard Gaussian and will be further justified in the next section. We can notice that the Kullback-Leibler divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271106ba",
   "metadata": {},
   "source": [
    "<img src=\"./img/vae/vae9.png\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039a939",
   "metadata": {},
   "source": [
    "### Intuitions about the regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d87630",
   "metadata": {},
   "source": [
    "* The regularity that is expected from the latent space in order to make generative process possible can be expressed through two main properties: continuity (two close points in the latent space should not give two completely different contents once decoded) and completeness (for a chosen distribution, a point sampled from the latent space should give “meaningful” content once decoded)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3743a7",
   "metadata": {},
   "source": [
    "<img src=\"./img/vae/vae10.png\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45495864",
   "metadata": {},
   "source": [
    "* With this regularisation term, we prevent the model to encode data far apart in the latent space and encourage as much as possible returned distributions to “overlap”, satisfying this way the expected continuity and completeness conditions. Naturally, as for any regularisation term, this comes at the price of a higher reconstruction error on the training data. The tradeoff between the reconstruction error and the KL divergence can however be adjusted and we will see in the next section how the expression of the balance naturally emerge from our formal derivation.\n",
    "* To conclude this subsection, we can observe that continuity and completeness obtained with regularisation tend to create a “gradient” over the information encoded in the latent space. For example, a point of the latent space that would be halfway between the means of two encoded distributions coming from different training data should be decoded in something that is somewhere between the data that gave the first distribution and the data that gave the second distribution as it may be sampled by the autoencoder in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e968de2",
   "metadata": {},
   "source": [
    "<img src=\"./img/vae/vae11.png\" height=\"600\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
